{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch scikit-learn pandas tqdm nltk nlpaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "78P5vbJJEXHU",
        "outputId": "e2c41055-2af9-4672-fb4a-b432b46bed38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m654.2/664.8 MB\u001b[0m \u001b[31m148.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
            "    unknown package:\n",
            "        Expected sha256 165764f44ef8c61fcdfdfdbe769d687e06374059fbb388b6c89ecb0e28793a6f\n",
            "             Got        4b6a0c3a1900b04ff63cb8709bacba28e27001bd4f7c0357e29d6def0ce5abcd\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nlpaug unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ebQtWxuJROJo",
        "outputId": "0026fa49-e5a4-49b3-cb4b-699f57542848"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.11/dist-packages (1.1.11)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.12.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import nlpaug.augmenter.word as naw\n",
        "import unidecode\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "spanish_stopwords = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "# Set device for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Spanish stopwords for TF-IDF\n",
        "spanish_stopwords = stopwords.words('spanish')"
      ],
      "metadata": {
        "id": "y-OVqX7QEZHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e3fdb5-d7e4-481d-8286-d3a16e2950cb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "def fix_label_format(label_str):\n",
        "    label_list = list(map(int, label_str.strip(\"[]\").split()))  # Convert to list of integers\n",
        "    return np.array(label_list)  # Convert to NumPy array for better compatibility\n",
        "\n",
        "# Apply function\n",
        "train_df[\"labels\"] = train_df[\"labels\"].astype(str).apply(fix_label_format)\n",
        "\n",
        "# Verify conversion\n",
        "print(train_df[\"labels\"].head())\n",
        "print(type(train_df[\"labels\"][0]))  # Should output <class 'numpy.ndarray'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9j5uDeFEbJe",
        "outputId": "ed0ea357-b694-45c3-ccbf-4703f19e141a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [0, 0, 0, 0]\n",
            "1    [0, 0, 1, 0]\n",
            "2    [0, 0, 0, 0]\n",
            "3    [0, 0, 0, 0]\n",
            "4    [0, 0, 0, 0]\n",
            "Name: labels, dtype: object\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clinical_terms = {\n",
        "    \"ACV\": \"Accidente cerebrovascular\",\n",
        "    \"AFG\": \"Alfa-fetoproteína\",\n",
        "    \"AFP\": \"Alfa-fetoproteína\",\n",
        "    \"AINE\": \"Antiinflamatorio no esteroideo\",\n",
        "    \"AINES\": \"Antiinflamatorios no esteroides\",\n",
        "    \"ALT\": \"Alanina aminotransferasa\",\n",
        "    \"ANA\": \"Anticuerpos antinucleares\",\n",
        "    \"ANCA\": \"Anticuerpos anticitoplasma de neutrófilos\",\n",
        "    \"AST\": \"Aspartato aminotransferasa\",\n",
        "    \"AVC\": \"Accidente vascular cerebral\",\n",
        "    \"AVR\": \"Reemplazo valvular aórtico\",\n",
        "    \"BAL\": \"Lavado broncoalveolar\",\n",
        "    \"BID\": \"Dos veces al día (bis in die)\",\n",
        "    \"BNP\": \"Péptido natriurético cerebral\",\n",
        "    \"BUN\": \"Nitrógeno ureico en sangre\",\n",
        "    \"CABG\": \"Injerto de derivación de arteria coronaria (bypass coronario)\",\n",
        "    \"CK\": \"Creatina quinasa\",\n",
        "    \"CMV\": \"Citomegalovirus\",\n",
        "    \"CPAP\": \"Presión positiva continua en la vía aérea\",\n",
        "    \"CRP\": \"Proteína C reactiva\",\n",
        "    \"CSF\": \"Líquido cefalorraquídeo\",\n",
        "    \"CT\": \"Tomografía computarizada\",\n",
        "    \"CVC\": \"Catéter venoso central\",\n",
        "    \"DLP\": \"Dislipidemia\",\n",
        "    \"DM\": \"Diabetes mellitus\",\n",
        "    \"DMI\": \"Diabetes mellitus tipo I\",\n",
        "    \"DMII\": \"Diabetes mellitus tipo II\",\n",
        "    \"DTC\": \"Disfunción tiroidea congénita\",\n",
        "    \"EAM\": \"Enfermedad arterial miocárdica\",\n",
        "    \"ECG\": \"Electrocardiograma\",\n",
        "    \"ECV\": \"Enfermedad cerebrovascular\",\n",
        "    \"EEG\": \"Electroencefalograma\",\n",
        "    \"EPOC\": \"Enfermedad pulmonar obstructiva crónica\",\n",
        "    \"ESR\": \"Velocidad de sedimentación globular\",\n",
        "    \"FAME\": \"Fármacos antirreumáticos modificadores de la enfermedad\",\n",
        "    \"FEVI\": \"Fracción de eyección del ventrículo izquierdo\",\n",
        "    \"FLOT\": \"Fluorouracilo, leucovorina, oxaliplatino, docetaxel (quimioterapia)\",\n",
        "    \"FRCV\": \"Factores de riesgo cardiovascular\",\n",
        "    \"GGT\": \"Gamma-glutamiltransferasa\",\n",
        "    \"HAA\": \"Hemorragia alveolar aguda\",\n",
        "    \"HCG\": \"Gonadotropina coriónica humana\",\n",
        "    \"HDL\": \"Lipoproteína de alta densidad\",\n",
        "    \"HIV\": \"Virus de inmunodeficiencia humana\",\n",
        "    \"HNF\": \"Hepatonefritis fulminante\",\n",
        "    \"HTA\": \"Hipertensión arterial\",\n",
        "    \"IAM\": \"Infarto agudo de miocardio\",\n",
        "    \"IC\": \"Insuficiencia cardíaca\",\n",
        "    \"ICU\": \"Unidad de cuidados intensivos\",\n",
        "    \"IGA\": \"Inmunoglobulina A\",\n",
        "    \"IGE\": \"Inmunoglobulina E\",\n",
        "    \"IGG\": \"Inmunoglobulina G\",\n",
        "    \"IM\": \"Intramuscular\",\n",
        "    \"INR\": \"Índice internacional normalizado\",\n",
        "    \"IPAP\": \"Presión inspiratoria positiva en la vía aérea\",\n",
        "    \"IV\": \"Intravenoso\",\n",
        "    \"IVH\": \"Hemorragia intraventricular\",\n",
        "    \"LDH\": \"Lactato deshidrogenasa\",\n",
        "    \"LDL\": \"Lipoproteína de baja densidad\",\n",
        "    \"MARSA\": \"Staphylococcus aureus resistente a meticilina (MRSA)\",\n",
        "    \"MN\": \"Meningitis\",\n",
        "    \"MPO\": \"Mieloperoxidasa\",\n",
        "    \"MRI\": \"Imagen por resonancia magnética\",\n",
        "    \"MSI\": \"Inestabilidad microsatelital\",\n",
        "    \"NSAID\": \"Antiinflamatorios no esteroides\",\n",
        "    \"OIT\": \"Oxigenoterapia hiperbárica\",\n",
        "    \"PA\": \"Presión arterial\",\n",
        "    \"PAF\": \"Paroxismo auricular fibrilatorio\",\n",
        "    \"PCR\": \"Proteína C reactiva\",\n",
        "    \"PCT\": \"Procalcitonina\",\n",
        "    \"PET\": \"Tomografía por emisión de positrones\",\n",
        "    \"PO\": \"Vía oral\",\n",
        "    \"PRN\": \"Según sea necesario (pro re nata)\",\n",
        "    \"PSA\": \"Antígeno prostático específico\",\n",
        "    \"PTH\": \"Parathormona\",\n",
        "    \"QD\": \"Cada día (quaque die)\",\n",
        "    \"QID\": \"Cuatro veces al día (quater in die)\",\n",
        "    \"QT\": \"Quimioterapia\",\n",
        "    \"RAST\": \"Prueba radioalergosorbente\",\n",
        "    \"RB\": \"Retinoblastoma\",\n",
        "    \"RCP\": \"Reanimación cardiopulmonar\",\n",
        "    \"SC\": \"Subcutáneo\",\n",
        "    \"SIDA\": \"Síndrome de inmunodeficiencia adquirida\",\n",
        "    \"SNC\": \"Sistema nervioso central\",\n",
        "    \"TAC\": \"Tomografía axial computarizada\",\n",
        "    \"TARV\": \"Terapia antirretroviral\",\n",
        "    \"TB\": \"Tuberculosis\",\n",
        "    \"TG\": \"Triglicéridos\",\n",
        "    \"TGO\": \"Transaminasa glutámico oxalacética\",\n",
        "    \"TGP\": \"Transaminasa glutámico pirúvica\",\n",
        "    \"TID\": \"Tres veces al día (ter in die)\",\n",
        "    \"TNF\": \"Factor de necrosis tumoral\",\n",
        "    \"TSH\": \"Hormona estimulante de la tiroides\",\n",
        "    \"UCI\": \"Unidad de cuidados intensivos\",\n",
        "    \"UEI\": \"Urea en orina de 24 horas\",\n",
        "    \"VCI\": \"Vena cava inferior\",\n",
        "    \"VCM\": \"Volumen corpuscular medio\",\n",
        "    \"VEGF\": \"Factor de crecimiento endotelial vascular\",\n",
        "    \"VHB\": \"Virus de la hepatitis B\",\n",
        "    \"VHC\": \"Virus de la hepatitis C\",\n",
        "    \"VRS\": \"Virus respiratorio sincitial\",\n",
        "}\n",
        "\n",
        "def expand_medical_terms(text):\n",
        "    for term, expanded in clinical_terms.items():\n",
        "        text = re.sub(r\"\\b\" + re.escape(term) + r\"\\b\", expanded, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "train_df[\"text\"] = train_df[\"text\"].apply(expand_medical_terms)\n",
        "test_df[\"text\"] = test_df[\"text\"].apply(expand_medical_terms)"
      ],
      "metadata": {
        "id": "btNlCN1ueaLM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if isinstance(text, list):  # Convert list to string if necessary\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    if not isinstance(text, str):  # Ensure it's a string\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-záéíóúüñ\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = \" \".join([word for word in text.split() if word not in spanish_stopwords])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n",
        "test_df[\"text\"] = test_df[\"text\"].apply(clean_text)\n",
        "\n",
        "# Diacritic Normalization\n",
        "def normalize_text(text):\n",
        "    return unidecode.unidecode(text)\n",
        "\n",
        "train_df[\"text\"] = train_df[\"text\"].apply(normalize_text)\n",
        "test_df[\"text\"] = test_df[\"text\"].apply(normalize_text)\n",
        "\n",
        "# Lemmatization\n",
        "!python -m spacy download es_core_news_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")  # Ensure you have this model installed\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "train_df[\"text\"] = train_df[\"text\"].apply(lemmatize_text)\n",
        "test_df[\"text\"] = test_df[\"text\"].apply(lemmatize_text)\n",
        "\n",
        "# Display samples\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T0R8G9wMd2Gm",
        "outputId": "3464b8a8-a35d-4063-b92b-502dfa459935",
        "collapsed": true
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                               text        labels\n",
              "0  train_0  presentar caso paciente ano antecedente artrit...  [0, 0, 0, 0]\n",
              "1  train_1  describir caso clinico escolar sexo masculino ...  [0, 0, 1, 0]\n",
              "2  train_2  hombre ano llego servicio urgencias presunto i...  [0, 0, 0, 0]\n",
              "3  train_3  mujer ano natural india residente espana dos a...  [0, 0, 0, 0]\n",
              "4  train_4  presentar caso paciente mujer ano clinico fieb...  [0, 0, 0, 0]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-896bf295-4dac-4485-b3c1-193d24cdf88f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>presentar caso paciente ano antecedente artrit...</td>\n",
              "      <td>[0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>describir caso clinico escolar sexo masculino ...</td>\n",
              "      <td>[0, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>hombre ano llego servicio urgencias presunto i...</td>\n",
              "      <td>[0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>mujer ano natural india residente espana dos a...</td>\n",
              "      <td>[0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>presentar caso paciente mujer ano clinico fieb...</td>\n",
              "      <td>[0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-896bf295-4dac-4485-b3c1-193d24cdf88f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-896bf295-4dac-4485-b3c1-193d24cdf88f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-896bf295-4dac-4485-b3c1-193d24cdf88f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed98ec9b-0679-425d-a98a-1f1c3ab6d031\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed98ec9b-0679-425d-a98a-1f1c3ab6d031')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed98ec9b-0679-425d-a98a-1f1c3ab6d031 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 502,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 502,\n        \"samples\": [\n          \"train_268\",\n          \"train_73\",\n          \"train_289\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 502,\n        \"samples\": [\n          \"antecedente personal var ano alergia medicamentosa conocido antecedente cardiologico conocido enfermedad behet aneurisma pulmonar pulmon derecho intervencion quirurgico hepatitis b cronico tratamiento domicilio aas mg peginterferon alfa entecavir enfermedad actual acudir astenia generalizado hacer dia dolor toracico relacion esfuerzo ceder inmediatamente detener \\u00e9l ortopnea disneo paroxistico nocturno presincope fiebre sensacion distermico dias previo exploracion fisico presion arterial mmhg fc lpm ta c consciente orientado colaborador buen general eupneico aire ambiente cabezar cuelo ivy auscultacion cardiaco ritmico soplo auscultacion pulmonar murmullo vesicular conservar ruido sobreanadido abdomir blar depresible doloroso organomegalia extremidad edema dato trombosis ven\\u00f3s profundo prueba complementario electrocardiograma bloqueo av alto grado conduccion qr ms eje qrs bloqueo bifascicular brdhh hai arritmia sinusal ventriculofasico radiografiar torax dato patologiar pleuropulmonar agudo perdido volumen pulmonar derecho material densidad metalica porcion inferior hemitorax ipsilateral relacion antecedente quirurgico silueta cardiopericardico tamano normal calcificaci\\u00f3n anular proyectar silueta cardiaca presente estudio previo analitico leucocito ul n l hb gdl plaquetas ul indice internacional normalizado tppa ratio gluc\\u00f3s mgdl urea mgdl creat mgdl na k troponina i ngdl ecocardiograma ventriculo izquierdo dilatado fraccion eyeccion preservado valvula aortico trivalir buen apertura insuficiencia ligero central auricula izquierdo dilatado valvula mitral buen apertura insuficiencia mitral diastolico raiz aortico dilatado cavidad derecho dilatada buen contractilidad ventriculo derecho insuficiencia tricuspidea ligero derrame pericardico coronariografiar tronco coronario izquierdo lesi\\u00f3n descendente anterior presentar dilatacion aneurismatico calcificado pared nivel proximal enlentecimiento flujo distal circunflejo presentar nivel proximal dilatacion aneurismatica gran tamano calcificado trombosis parcial luz condicionar enlentecimiento flujo nivel distal coronario derecho lesi\\u00f3n aortografia presentar dilatacion aneurismatico tc coronario aneurisma coronario sacular circunflejar parcialmente trombosado cuello mm descendente anterior trombosado asentado segmento arteria displasica mm longitud evolucion paciente ano ingresar unidad coronario bloqueo av qrs ancho sintoma compatible anginar esfuerzo reciente comienzo llevar cabo coronariografia evidencia arterio descendente anterior dilatacion aneurismatico calcificado pared nivel proximal enlentecimiento flujo distal circunfleja proximal dilatacion aneurismatica gran tamano calcificado trombosis parcial luz condicionar enlentecimiento flujo nivel distal comentar caso sesion medico quirurgico decidir cirugia coronario previo implante marcapaso definitivo comentar sesion conveniencia suspender temporalmente tratamiento peginterferon alfa car mejorar posible cifra leucocito recuento plaquetario cara intervencion respecto valorado hepatologia recomendar suspender peginterferon cara cirugiir reanudar \\u00e9l posible tras intervencion dia octubre realizar eef evidenciar \\u00e9l tratar bloqueo av infrahisiano posteriormente implantar marcapaso definitivo bicameral complicaci\\u00f3n control posimplante marcapaso mostrar adecuado funcionamiento dispositivo encontrar \\u00e9l estable cardiovascular asintomatico dato insuficiencia cardiaco autoriza alto hospitalaria ingreso proxima semana cara intervencion quirurgico cardiacir dia noviembre realizar forma programado cirugia revascularizacion coronario doble arteria mamario bajo cec realizar bypass arterio mamario izquierda descendente anterior arteria mamario derecho obtusa marginal asimismo realizar ligadura distal proximal aneurisma descendente anterior intervencion complicaci\\u00f3n posoperatorio reanimacion incidencia diagnostico enfermedad behet bloqueo auriculoventricular infrahisiano implante marcapaso definitivo bicameral cardiopatia isquemico aneurisma coronario revascularizacion coronario doble arteria mamario descendente anterior obtusa marginal ligaduro aneurisma descendente anterior\",\n          \"anamnesis mujer ano embarazado semana consultar servicio urgencia dermatologiir presentar hacer dia lesi\\u00f3n puntiform dolorosa aparicion subita punto dedo paciente referir lesi\\u00f3n iniciar dedo mano extender \\u00e9l posteriormente dedo pi asocia adema molestia articular dia aparicion lesi\\u00f3n presento cuadro febril remitio hora paracetamol tras desarrollo cuadro cutaneo vuelto presentar fiebre presentar ninguno sintomatologia asociado antecedente personal inter destacar unicamente urticaria frigorar antecedent fotosensibilidad exploracion fisico papula puntiform eritematosa zona purpurica palpabl desaparecer digitopresion bien delimitada infiltrado localizado pulpejo distal dedo uno alteraci\\u00f3n mano pi normocoloreado normoperfundido exploraci\\u00f3n complementario primero momento caso impresionar vasculitis extraer analitico sangre periferico alteraci\\u00f3n excepto elevacion velocidad sedimentacion globular mm valor referenciar mm marcador inmunidad anticuerpo antinuclear crioaglutinina resultar negativo serologia virus hepatitis b indicar unicamente vacunacion asi mismo realizo biopsia cutaneo punch mm observar unico hallazgo hiperqueratosi paraqueratosis infiltrado inflamatorio cronico perivascular dermis superficial base resultado descarto diagnostico vasculitis sospechar diagnostico exantema guante calcetin realizo serologia virus ebstein barr parvovirus b enterovirus humano serologar parvoviru b igm positiva dos ocasi\\u00f3n separado semana diagnostico exantema guante calcetin parvovirus b tratamiento sintomatico paracetamol g cada hora equipo ginecologia pauto tratamiento aas mg cada hora evolucion primero momento lesi\\u00f3n compatible desarrollo vasculitis marcadores analitico negativo autoinmunidad crioaglutinina orientar diagnostico hacia exantema infeccioso erupcion papulopurpurico guante calcetin sospechar cuadro infeccioso caracteristica solicitar serologia veb enteroviru parvovirus b ser igm positiva ultimo dos ocasi\\u00f3n tratar \\u00e9l paciente embarazado poner contacto servicio ginecologio iniciar \\u00e9l seguimiento gineobstetrico paciente alto riesgo sucesivo revisi\\u00f3n ginecologica paciente desarrollado anemiar manifestaci\\u00f3n clinica infeccion agudo parvovirus tampoco objetivar signo anemiar fetal complicaci\\u00f3n derivado este\",\n          \"antecedente personal var ano buen salud habitual alergia medicamentosa tratamiento habitual fumador paquetesano bebedor leve casado comercial material electronico anamnesis noviembre comenzar fiebre tos expectoracion acudir s urgencia practicar rx torax poner manifiesto masa lobulo superior derecho lsd pulmon exploracion fisico ecog anodino prueba complementario analitico general cea ca virus inmunodeficiencia humano negativo tc tomografia computarizado toraxabdomenpelvis masa lsd mm adenopatia mediastinica paratraqueal derecho hiliar ipsilateral subcarinal mm quist simple hepatico segmento v ver vii broncoscopio normal broncoaspirado cepillado negativo bag masa lsd guiado tc adenocarcinoma pulmon egfr nativo alk traslocado ihq fish tomografia emision positr\\u00f3n tc tomografia emision positr\\u00f3n masa lsd xx cm suv adenopatia mediastinico derecho alto mm suv adenopatia prevascular mm suv adenopatia paratraqueal derecho alto mm suv adenopatia paratraqueal derecha bajo mm suv adenopatia hilar derecho suv segmento intravenosov hepatico imagen hiperactiva suv ecografia abdominal quist simple hepatico segmento resonancia magnetico hepatico quist hepatico segmento v ver vii viii ecobroncoscopia adenopatia menor cm nivel r puncion negativo adenopatia r positivo adenocarcinoma tc craneal normal diagnostico remitido serviciocon diagnostico adenocarcinoma pulmon lsd tanhiliar mediastinica ipsilateral m estadio iiia egfr nativo alk traslocado tratamiento iniciar tratamiento crizotinib mg cada hora forma continuo evolucion tras mes tratamiento realizar tc reevaluacion reduccion masa lsd ranguir respuesta parcial respuesta completo adenopatia paratraqueal derecha respuesta parcial adenopatiir hiliar derecha subcarinal presentar efecto secundario alteraci\\u00f3n visual formar aura predominio vespertino tras mes tto crizotinib realizar tomografia emision positr\\u00f3n tc objetivo respuesta completo metabolico adenopatia mediastinica hiliares respuesta parcial tumor respuesta remitir paciente s cirugia toracico salamanco valoracion quirurgico previo realizacion mediastinoscopia realizar mediastinoscopia nivel l r negativo realizar lobectomia lsd linfadenectomia r videotoracoscopio ypta mm pnm administra radioterapia complemetaria inicio fin dt gy fracci\\u00f3n efecto secundario hemoptisi odinofagia dermitis g realizar tc tap evidenciar enfermedad\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "# Identify rare labels\n",
        "label_counts = np.sum(np.vstack(train_df[\"labels\"].values), axis=0)\n",
        "rare_labels = [i for i, count in enumerate(label_counts) if count < 30]  # Threshold: Labels appearing <30 times\n",
        "\n",
        "# Synonym replacement augmentation\n",
        "aug = naw.SynonymAug(aug_src=\"wordnet\")\n",
        "\n",
        "def augment_text(text):\n",
        "    return aug.augment(text)\n",
        "\n",
        "# Apply augmentation on rare label samples\n",
        "augmented_texts = []\n",
        "augmented_labels = []\n",
        "\n",
        "for _, row in train_df.iterrows():\n",
        "    if any(row[\"labels\"][i] == 1 for i in rare_labels):\n",
        "        new_text = augment_text(row[\"text\"])\n",
        "        augmented_texts.append(new_text)\n",
        "        augmented_labels.append(row[\"labels\"])\n",
        "\n",
        "# Add augmented data\n",
        "aug_df = pd.DataFrame({\"text\": augmented_texts, \"labels\": augmented_labels})\n",
        "train_df = pd.concat([train_df, aug_df], ignore_index=True)\n",
        "\n",
        "print(f\"Dataset size after augmentation: {len(train_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9SlnVtQUT0V",
        "outputId": "92dffa3f-3355-40bc-af55-0911d57f0188"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size after augmentation: 551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dynamically determine the number of labels\n",
        "num_labels = len(train_df[\"labels\"][0])  # Get number of labels from first row\n",
        "label_cols = [f\"label_{i}\" for i in range(num_labels)]  # Create column names\n",
        "print(\"Available columns:\", train_df.columns)  # Check all column names\n",
        "print(\"Expected label columns:\", label_cols)  # Check generated label names\n",
        "\n",
        "# Ensure labels are properly split\n",
        "if \"labels\" in train_df.columns:\n",
        "    num_labels = len(train_df[\"labels\"][0])  # Get number of labels\n",
        "    label_cols = [f\"label_{i}\" for i in range(num_labels)]  # Generate column names\n",
        "\n",
        "    # Convert lists into separate columns\n",
        "    train_df[label_cols] = pd.DataFrame(train_df[\"labels\"].tolist(), index=train_df.index)\n",
        "\n",
        "    # Drop the original labels column\n",
        "    train_df.drop(columns=[\"labels\"], inplace=True)\n",
        "\n",
        "# Verify columns again\n",
        "print(\"Final label columns:\", train_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLNXM_rFuhFz",
        "outputId": "2aa1ddb7-ff9e-43ff-c136-8c90d5e158a7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns: Index(['id', 'text', 'labels'], dtype='object')\n",
            "Expected label columns: ['label_0', 'label_1', 'label_2', 'label_3']\n",
            "Final label columns: Index(['id', 'text', 'label_0', 'label_1', 'label_2', 'label_3'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
        "\n",
        "#tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokenize function\n",
        "#def tokenize_texts(texts, max_length=512):\n",
        "#    return tokenizer(\n",
        "#        texts.tolist(),\n",
        "#        padding=\"max_length\",\n",
        "#        truncation=True,\n",
        "#        max_length=max_length,\n",
        "#        return_tensors=\"pt\"\n",
        "#    )\n",
        "\n",
        "# Load tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "klwGg_OpEb0K"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClinicalDataset(Dataset):\n",
        "    def __init__(self, texts, labels=None, tokenizer=None, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels  # Already converted to float32\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[index],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            label_tensor = torch.tensor(np.array(self.labels[index], dtype=np.float32))  # Ensure float32 conversion\n",
        "            return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": label_tensor}\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n"
      ],
      "metadata": {
        "id": "QhUd93zSEfBc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get the label columns dynamically\n",
        "label_cols = [col for col in train_df.columns if col.startswith('label_')]\n",
        "\n",
        "# Extract label values as a NumPy array\n",
        "y_labels = train_df[label_cols].values\n",
        "\n",
        "# Perform Train-Validation Split\n",
        "#X_train, X_val, y_train, y_val = train_test_split(\n",
        "#    train_df['text'], y_labels,  # Use extracted NumPy array\n",
        "#    test_size=0.2, random_state=42, stratify=y_labels.sum(axis=1)  # Ensure proper stratification\n",
        "#)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for train_idx, val_idx in skf.split(train_df[\"text\"], y_labels.sum(axis=1)):  # Use sum of labels for stratification\n",
        "    X_train, X_val = train_df[\"text\"].iloc[train_idx], train_df[\"text\"].iloc[val_idx]\n",
        "    y_train, y_val = y_labels[train_idx], y_labels[val_idx]\n",
        "    break  # Use only first fold, or iterate for full k-fold training\n",
        "\n",
        "# Verify output before proceeding\n",
        "print(\"y_train shape:\", y_train.shape)  # Should be (num_samples, num_labels)\n",
        "print(\"y_train example:\", y_train[:5])  # Should contain lists of 0s and 1s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O06ICH1SEghY",
        "outputId": "8379d6e4-15e5-493d-c7ea-21875f65fb38"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train shape: (440, 4)\n",
            "y_train example: [[0 0 0 0]\n",
            " [0 0 1 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ClinicalDataset(X_train.tolist(), y_train, tokenizer)\n",
        "val_dataset = ClinicalDataset(X_val.tolist(), y_val, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "OMFwnQ2uub3C"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertMultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(BertMultiLabelClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        dropped = self.dropout(pooled_output)\n",
        "        return self.classifier(dropped)  # No sigmoid since BCEWithLogitsLoss expects raw logits\n",
        "\n",
        "model = BertMultiLabelClassifier(MODEL_NAME, num_labels)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLBFN34OEh-V",
        "outputId": "ca609161-e3c9-472c-ae41-c0ec669fca52"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure labels are numeric\n",
        "train_df[label_cols] = train_df[label_cols].astype(np.float32)\n",
        "\n",
        "# Compute normalized class weights\n",
        "class_weights = train_df[label_cols].sum().values\n",
        "class_weights = class_weights / class_weights.sum()  # Normalize sum to 1\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).cuda()\n",
        "\n",
        "LABEL_SMOOTHING = 0.1  # Small smoothing factor\n",
        "\n",
        "def smooth_labels(labels, epsilon=LABEL_SMOOTHING):\n",
        "    return labels * (1 - epsilon) + (epsilon / labels.shape[1])\n",
        "\n",
        "# Apply label smoothing before training\n",
        "y_train = smooth_labels(y_train)\n",
        "y_val = smooth_labels(y_val)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(weight=class_weights)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "print(\"Class weights:\", class_weights)  # Verify output\n"
      ],
      "metadata": {
        "id": "LIUJhM4yEjaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7ab567-317d-4657-8458-44565fabc320"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: tensor([0.2157, 0.1716, 0.3480, 0.2647], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
        "    best_f1 = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "            input_ids = batch[\"input_ids\"].cuda()\n",
        "            attention_mask = batch[\"attention_mask\"].cuda()\n",
        "            labels = batch[\"labels\"].cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Training Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].cuda()\n",
        "                attention_mask = batch[\"attention_mask\"].cuda()\n",
        "                labels = batch[\"labels\"].cuda()\n",
        "\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "                all_preds.append(preds)\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        # Convert lists to NumPy arrays before computing F1-score\n",
        "        all_preds = np.vstack(all_preds)  # Stack predictions vertically\n",
        "        all_labels = np.vstack(all_labels) # Stack labels vertically\n",
        "\n",
        "        # Compute macro F1-score\n",
        "        val_f1 = f1_score(all_labels, all_preds > 0.3, average=\"macro\") # Threshold predictions\n",
        "        print(f\"Epoch {epoch+1} - Macro F1: {val_f1}\")\n",
        "\n",
        "        # Save model if it's the best so far\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(\"Best model saved!\")"
      ],
      "metadata": {
        "id": "jxboZs6cEkeZ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)"
      ],
      "metadata": {
        "id": "YdoTQFllEoQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2712aef-dba4-4b10-c935-49bc4b93260d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:42<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.0002401777482274073\n",
            "Epoch 1 - Macro F1: 0.8604532163742691\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:44<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training Loss: 0.0002357519999019463\n",
            "Epoch 2 - Macro F1: 0.8604532163742691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:43<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training Loss: 0.000229266016156709\n",
            "Epoch 3 - Macro F1: 0.8765350877192981\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:44<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training Loss: 0.01123519326991465\n",
            "Epoch 4 - Macro F1: 0.5118898543317149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:43<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training Loss: 0.04577396788465028\n",
            "Epoch 5 - Macro F1: 0.67804446193193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:44<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training Loss: 0.019355015525467357\n",
            "Epoch 6 - Macro F1: 0.7494172494172494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:43<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training Loss: 0.008019773992286486\n",
            "Epoch 7 - Macro F1: 0.8649546106067846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:44<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training Loss: 0.003358731254808266\n",
            "Epoch 8 - Macro F1: 0.8708074534161491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:44<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training Loss: 0.0031401811194055797\n",
            "Epoch 9 - Macro F1: 0.8976897117629383\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:44<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training Loss: 0.001024198009293865\n",
            "Epoch 10 - Macro F1: 0.9137715831079676\n",
            "Best model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ClinicalDataset(test_df[\"text\"].tolist(), None, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "test_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        input_ids = batch[\"input_ids\"].cuda()\n",
        "        attention_mask = batch[\"attention_mask\"].cuda()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "        test_preds.append(preds)\n",
        "\n",
        "test_preds = np.vstack(test_preds)\n",
        "test_preds = (test_preds > 0.5).astype(int)  # Convert to binary labels\n",
        "\n",
        "# Save Submission File\n",
        "submission = pd.DataFrame(test_preds, columns=label_cols)\n",
        "submission.insert(0, \"id\", test_df[\"id\"])\n",
        "submission.insert(1, \"text\", test_df[\"text\"])\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwcuNbbsg0HF",
        "outputId": "5aa69637-2295-4f9d-b1d3-e9d280b97b3f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-8e7b18680c70>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n",
            "100%|██████████| 31/31 [00:08<00:00,  3.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure predictions are formatted as comma-separated values inside square brackets\n",
        "formatted_preds = [\"[\" + \", \".join(map(str, row)) + \"]\" for row in test_preds]\n",
        "\n",
        "# Save Submission File with the corrected format\n",
        "submission = pd.DataFrame({\"id\": test_df[\"id\"], \"text\": test_df[\"text\"], \"pred\": formatted_preds})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"Submission file saved as submission.csv!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSB9jt48hHja",
        "outputId": "23501285-2650-4ef8-a2c1-4d6a2d2b8d46"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved as submission.csv!\n"
          ]
        }
      ]
    }
  ]
}